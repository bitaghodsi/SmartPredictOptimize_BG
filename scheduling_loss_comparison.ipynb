{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "Install and import all required Python packages, including SymPy, PyTorch, CVXPY, and supporting libraries for optimization and preprocessing.\n"
      ],
      "metadata": {
        "id": "W0ZXMAvZxCMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ],
      "metadata": {
        "id": "VplfMpWcvrpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83-IHbM2QMml"
      },
      "outputs": [],
      "source": [
        "# Uninstall all existing versions of sympy\n",
        "!pip uninstall sympy -y\n",
        "\n",
        "# Install sympy version 1.12\n",
        "!pip install sympy==1.12\n",
        "\n",
        "# Import sympy and check the installed version\n",
        "import sympy\n",
        "print(\"Sympy version:\", sympy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install cvxpylayers for differentiable convex optimization layers\n",
        "!pip install cvxpylayers\n",
        "\n",
        "# Install cvxpy for convex optimization modeling\n",
        "!pip install cvxpy"
      ],
      "metadata": {
        "id": "aldampiupH4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading\n",
        "Read the appointment scheduling dataset from a CSV file. The data contains scheduled visit times and other features for patient appointments.\n"
      ],
      "metadata": {
        "id": "y650D3bJxVrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path3 = '/content/scheduled_visits_Bita.csv'\n",
        "df = pd.read_csv(file_path3)"
      ],
      "metadata": {
        "id": "pti1DBJ6Q0tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Engineering\n",
        "Extract useful date and time components from the `scheduled_time` column. Also create new features like time of day, day of week, and whether the appointment falls on a weekend.\n"
      ],
      "metadata": {
        "id": "g-jt6tULxa8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['scheduled_time'] = pd.to_datetime(df['scheduled_time'])\n",
        "\n",
        "# Time components\n",
        "df['hour'] = df['scheduled_time'].dt.hour\n",
        "df['minute'] = df['scheduled_time'].dt.minute\n",
        "\n",
        "# Date components\n",
        "df['day'] = df['scheduled_time'].dt.day\n",
        "df['month'] = df['scheduled_time'].dt.month\n",
        "df['year'] = df['scheduled_time'].dt.year\n",
        "df['week_of_year'] = df['scheduled_time'].dt.isocalendar().week.astype(int)\n",
        "df['day_of_year'] = df['scheduled_time'].dt.dayofyear\n",
        "\n",
        "# Day of week (0 = Monday, 6 = Sunday)\n",
        "df['day_of_week'] = df['scheduled_time'].dt.dayofweek\n",
        "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Time of day bins\n",
        "df['time_of_day'] = pd.cut(\n",
        "    df['hour'],\n",
        "    bins=[-1, 6, 12, 17, 21, 24],\n",
        "    labels=['night', 'morning', 'afternoon', 'evening', 'late night']\n",
        ")\n"
      ],
      "metadata": {
        "id": "yHbJJ8enQ06y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encode categorical features\n",
        "categorical_cols = ['floor_id', 'department', 'diagnosis', 'appointment_type', 'link_flag', 'time_of_day']\n",
        "for col in categorical_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])"
      ],
      "metadata": {
        "id": "XaG8llPtQ1AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Subset Selection\n",
        "Focus on a subset of data where the event type is 'Exam' and further filter by month (e.g., January). Select the features and target variable (`duration_min`) to be used in modeling.\n"
      ],
      "metadata": {
        "id": "q1zqR6yWxq1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_exam = df[df['event_desc'] == 'Exam']\n",
        "df_exam_sub = df_exam[df_exam['month'] == 1]"
      ],
      "metadata": {
        "id": "HlErXSBeROXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = [\n",
        "    # 'time_of_day', 'day_of_week', 'is_weekend', 'day', 'month',\n",
        "    'floor_id', 'department',\n",
        "    # 'diagnosis',\n",
        "    'appointment_type', 'link_flag',\n",
        "    'scheduled_duration_min',\n",
        "    'duration_min',\n",
        "]\n",
        "\n",
        "df_exam_sub_cols = df_exam_sub[selected_columns]"
      ],
      "metadata": {
        "id": "1qVpyBdJRS7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where duration_min is missing\n",
        "df_NN = df_exam_sub_cols.dropna(subset=['duration_min'])\n",
        "\n",
        "# Select features and target variable\n",
        "X = df_NN[['appointment_type', 'department', 'floor_id']].values\n",
        "y = df_NN['duration_min'].values\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "KGV17TGLRTDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of input features\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Normalize target values to improve training stability\n",
        "y_train_max = y_train.max()\n",
        "y_train = y_train / y_train_max\n",
        "y_test = y_test / y_train_max"
      ],
      "metadata": {
        "id": "pwoVfW9CuO7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. SCT Evaluation Functions\n",
        "Define helper functions to compute the sum of completion times (SCT) given a task ordering, and to measure how closely a model's predictions match the optimal ordering.\n"
      ],
      "metadata": {
        "id": "WVhJ50OLyT2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# SCT Evaluation\n",
        "# -------------------------------\n",
        "\n",
        "def calculate_sct_ordering(durations, ordering):\n",
        "    # Compute SCT for given ordering of durations\n",
        "    sorted_durations = durations[ordering]\n",
        "    return np.sum(np.cumsum(sorted_durations))\n",
        "\n",
        "\n",
        "def calculate_relative_sct_error(model, X_test, y_test):\n",
        "    # Compute predicted durations and compare SCT error with optimal ordering\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(torch.tensor(X_test, dtype=torch.float32)).squeeze().numpy()\n",
        "    pred_order = np.argsort(y_pred)\n",
        "    true_order = np.argsort(y_test)\n",
        "    pred_sct = calculate_sct_ordering(y_test, pred_order)\n",
        "    true_sct = calculate_sct_ordering(y_test, true_order)\n",
        "    return (pred_sct - true_sct) / (true_sct + 1e-8)"
      ],
      "metadata": {
        "id": "HQGhmb8Zocky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Definition\n",
        "Define a simple 3-layer neural network using PyTorch. The output uses a Softplus activation to ensure non-negative predictions (valid durations).\n"
      ],
      "metadata": {
        "id": "dcFJYLbsyZs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Neural Network Model\n",
        "# -------------------------------\n",
        "class SmallNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        # Simple 3-layer feedforward network with Softplus output\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Softplus()  # Ensures non-negative output\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "tpu8m-yro0z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Custom Loss Functions\n",
        "Implement loss functions for training. These are used to compare the effectiveness of training.\n"
      ],
      "metadata": {
        "id": "Bw1-qSHwyc9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Expected Regret Loss using sigmoid-based misordering probability\n",
        "# -------------------------------\n",
        "def expected_regret_loss_sigmoid(y_true, y_pred, num_pairs=200000):\n",
        "    N = y_true.size(0)\n",
        "\n",
        "    # Randomly sample pairs of indices\n",
        "    idx = torch.randint(0, N, (num_pairs, 2))\n",
        "    i, j = idx[:, 0], idx[:, 1]\n",
        "\n",
        "    # Compute prediction difference for each pair\n",
        "    diff_pred = y_pred[i] - y_pred[j]\n",
        "\n",
        "    # Estimate probability that the pair is misordered (i ranked above j incorrectly)\n",
        "    misorder_prob = torch.sigmoid(diff_pred / 0.1)\n",
        "\n",
        "    # Compute regret only when the true order is violated\n",
        "    regret = torch.relu((y_true[j] - y_true[i]) * misorder_prob)\n",
        "\n",
        "    # Return the average regret over all sampled pairs\n",
        "    return torch.mean(regret)\n",
        "\n",
        "# -------------------------------\n",
        "# Rank loss based on pairwise comparisons and sigmoid scoring\n",
        "# -------------------------------\n",
        "def rank_loss(y_true, y_pred, num_pairs=200000):\n",
        "    N = y_true.size(0)\n",
        "\n",
        "    # Randomly sample pairs of indices\n",
        "    idx = torch.randint(0, N, (num_pairs, 2))\n",
        "    i, j = idx[:, 0], idx[:, 1]\n",
        "\n",
        "    # Keep only pairs where true label of i < j\n",
        "    mask = (y_true[i] < y_true[j])\n",
        "    i_masked = i[mask]\n",
        "    j_masked = j[mask]\n",
        "\n",
        "    # Return zero if no valid pairs are found\n",
        "    if i_masked.numel() == 0:\n",
        "        return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "    # Compute pairwise differences in predictions\n",
        "    diff_pred = y_pred[j_masked] - y_pred[i_masked]\n",
        "\n",
        "    # Apply negative log-sigmoid loss\n",
        "    log_s = torch.log(torch.sigmoid(diff_pred) + 1e-12)\n",
        "    return -torch.mean(log_s)\n",
        "\n",
        "# -------------------------------\n",
        "# Mean Squared Error loss (standard regression loss)\n",
        "# -------------------------------\n",
        "def mse_loss(y_true, y_pred, num_pairs=None):  # num_pairs unused, just for API compatibility\n",
        "    return torch.mean((y_true - y_pred) ** 2)"
      ],
      "metadata": {
        "id": "fMtrlVANpx82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# SPO+ Loss via CVXPY Layer\n",
        "# -------------------------------\n",
        "def create_spo_plus_layer(n_jobs):\n",
        "    # Define and return the SPO+ optimization layer\n",
        "    x = cp.Variable(n_jobs)\n",
        "    c = cp.Parameter(n_jobs)\n",
        "    problem = cp.Problem(cp.Minimize(c @ x), [x >= 0, cp.sum(x) == n_jobs])\n",
        "    return CvxpyLayer(problem, parameters=[c], variables=[x])\n",
        "\n",
        "def spo_plus_loss_true(y_true, y_pred, spo_layer):\n",
        "    # Calculate SPO+ surrogate loss minus the true SPO loss\n",
        "    n = y_true.size(0)\n",
        "    device = y_true.device\n",
        "    true_order = torch.argsort(y_true)\n",
        "    true_completion_times = torch.arange(1, n + 1, dtype=torch.float32).to(device)\n",
        "    spo_true_loss = torch.sum(y_true[true_order] * true_completion_times)\n",
        "\n",
        "    x_opt, = spo_layer(y_pred)\n",
        "    surrogate = 2 * torch.sum(y_pred * x_opt) - torch.sum(y_true * x_opt)\n",
        "\n",
        "    return surrogate - spo_true_loss"
      ],
      "metadata": {
        "id": "-57RzHFPyqkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Training Loop\n",
        "Train a model using any loss function and evaluate it using SCT-based metrics. Early stopping is applied if the loss does not improve.\n"
      ],
      "metadata": {
        "id": "p06huWYOy8tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Training loop for a model using a custom loss function\n",
        "# -------------------------------\n",
        "\n",
        "def train_model(model, optimizer, loss_fn, X_train, y_train, X_test, y_test,\n",
        "                num_iterations=1000, num_pairs=200000, eval_interval=100,\n",
        "                patience=200, min_delta=1e-5):\n",
        "\n",
        "    model.train()\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    no_improvement_counter = 0\n",
        "    sct_diffs = []\n",
        "    steps = []\n",
        "\n",
        "    for step in range(num_iterations):\n",
        "        # Forward + backward pass\n",
        "        optimizer.zero_grad()\n",
        "        y_pred_train = model(X_train_tensor).squeeze()\n",
        "        train_loss = loss_fn(y_train_tensor, y_pred_train, num_pairs)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate model every eval_interval steps (or last step)\n",
        "        if step % eval_interval == 0 or step == num_iterations - 1:\n",
        "            sct_diff = calculate_relative_sct_error(\n",
        "                model, X_test, y_test,\n",
        "                model_type=\"Real Model\", print_results=False\n",
        "            )\n",
        "            print(f\"Step {step:4d} | Train Loss: {train_loss.item():.4f} | sct_error: {sct_diff:.2f}\")\n",
        "            sct_diffs.append(sct_diff)\n",
        "            steps.append(step)\n",
        "\n",
        "        # Early stopping if no significant improvement\n",
        "        if best_loss - train_loss.item() > min_delta:\n",
        "            best_loss = train_loss.item()\n",
        "            no_improvement_counter = 0\n",
        "        else:\n",
        "            no_improvement_counter += 1\n",
        "            if no_improvement_counter >= patience:\n",
        "                print(f\"Early stopping at step {step}, best train loss: {best_loss:.6f}\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation: compute test MSE\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = model(torch.tensor(X_test, dtype=torch.float32)).squeeze().numpy()\n",
        "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "    print(f\"Final Test MSE: {test_mse:.4f}\")\n",
        "\n",
        "    return steps, sct_diffs"
      ],
      "metadata": {
        "id": "BPfGlQhAs3Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Training Loop (Mini-batch SPO+)\n",
        "# -------------------------------\n",
        "def train_model_spo(model, optimizer, X_train, y_train, X_test, y_test,\n",
        "                    num_iterations=1000, batch_size=1000,\n",
        "                    eval_interval=100, min_delta=1e-9, patience=1000):\n",
        "    model.train()\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).squeeze()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    steps, rel_sct_errors = [], []\n",
        "    no_improvement_counter = 0\n",
        "    last_logged_step = -1\n",
        "\n",
        "    for step in range(num_iterations):\n",
        "        # Sample a mini-batch\n",
        "        idx = torch.randperm(len(X_train_tensor))[:batch_size]\n",
        "        X_batch = X_train_tensor[idx]\n",
        "        y_batch = y_train_tensor[idx]\n",
        "\n",
        "        # Create new CVXPY layer for this batch size\n",
        "        spo_layer = create_spo_plus_layer(batch_size)\n",
        "\n",
        "        # Forward + backward pass\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch).squeeze()\n",
        "        loss = spo_plus_loss_true(y_batch, y_pred, spo_layer)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluation step\n",
        "        if step % eval_interval == 0:\n",
        "            rel_error = calculate_relative_sct_error(model, X_test, y_test)\n",
        "            print(f\"Step {step:4d} | SPO+ Loss: {loss.item():.4f} | Rel. SCT Error: {rel_error:.4f}\")\n",
        "            steps.append(step)\n",
        "            rel_sct_errors.append(rel_error)\n",
        "            last_logged_step = step\n",
        "\n",
        "        # Check for improvement\n",
        "        if best_loss - loss.item() > min_delta:\n",
        "            best_loss = loss.item()\n",
        "            no_improvement_counter = 0\n",
        "        else:\n",
        "            no_improvement_counter += 1\n",
        "            if no_improvement_counter >= patience:\n",
        "                print(f\"Early stopping at step {step}\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation if last step wasn't logged\n",
        "    if last_logged_step != step:\n",
        "        rel_error = calculate_relative_sct_error(model, X_test, y_test)\n",
        "        print(f\"Final Step {step:4d} | SPO+ Loss: {loss.item():.4f} | Rel. SCT Error: {rel_error:.4f}\")\n",
        "        steps.append(step)\n",
        "        rel_sct_errors.append(rel_error)\n",
        "\n",
        "    return steps, rel_sct_errors"
      ],
      "metadata": {
        "id": "QMhjnW5Do-fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Comparison of All Loss Functions\n",
        "Plot relative SCT error curves for all trained models. Final performance is highlighted and extended to the end of training for visual comparison.\n"
      ],
      "metadata": {
        "id": "pKcbKAcP2KPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input feature size\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate the model using different loss functions\n",
        "for loss_name, loss_fn in {\n",
        "    'Regret Loss (Sigmoid)': expected_regret_loss_sigmoid,\n",
        "    'Rank Loss': rank_loss,\n",
        "    'MSE Loss': mse_loss\n",
        "}.items():\n",
        "    print(f\"\\nTraining with {loss_name}\")\n",
        "\n",
        "    # Initialize a new model and optimizer for each loss function\n",
        "    model = SmallNN(input_size=input_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "    # Train the model and collect evaluation metrics\n",
        "    steps, sct_diffs = train_model(\n",
        "        model, optimizer, loss_fn,\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        num_iterations=2000, eval_interval=50\n",
        "    )\n",
        "\n",
        "    # Store the results\n",
        "    results[loss_name] = (steps, sct_diffs)"
      ],
      "metadata": {
        "id": "GiwUGjz5tpRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "steps, rel_errors = train_model_spo(model, optimizer, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "wCgM5eNgq3Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Assign fixed colors to each loss function\n",
        "colors = {\n",
        "    'Regret Loss (Sigmoid)': 'blue',\n",
        "    'Rank Loss': 'orange',\n",
        "    'MSE Loss': 'green',\n",
        "    'SPO+': 'red'\n",
        "}\n",
        "\n",
        "# Combine all results, including SPO+ results from earlier\n",
        "all_results = results.copy()\n",
        "all_results['SPO+'] = (steps, rel_errors)\n",
        "\n",
        "# Define maximum x-value for horizontal dashed line extension\n",
        "max_step = 2000\n",
        "\n",
        "# Plot each model's training curve\n",
        "for loss_name, (loss_steps, loss_errors) in all_results.items():\n",
        "    color = colors.get(loss_name, None)\n",
        "\n",
        "    # Plot the training curve\n",
        "    plt.plot(loss_steps, loss_errors, marker='o', label=loss_name, color=color)\n",
        "\n",
        "    # Mark and annotate the final point\n",
        "    final_step = loss_steps[-1]\n",
        "    final_error = loss_errors[-1]\n",
        "    plt.scatter(final_step, final_error, color=color)\n",
        "    plt.text(final_step, final_error, f\"{final_error:.3f}\", fontsize=8,\n",
        "             ha='right', va='bottom', color=color)\n",
        "\n",
        "    # Extend a dashed line to the right to compare final performance\n",
        "    if final_step < max_step:\n",
        "        plt.hlines(y=final_error, xmin=final_step, xmax=max_step,\n",
        "                   linestyles='dashed', colors=color, alpha=0.5)\n",
        "\n",
        "# Add labels, title, legend, and formatting\n",
        "plt.xlabel(\"Training Step\")\n",
        "plt.ylabel(\"Relative SCT Error\")\n",
        "plt.title(\"Comparison of Loss Functions\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MsHBDSeTtQD9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}